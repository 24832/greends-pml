{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlltQwsywCk4WNt30vR9O5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isa-ulisboa/greends-pml/blob/main/assignments/assignment_to_return_march_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical Machine Learning\n",
        "\n",
        "Masters in Green Data Science, ISA/ULisboa, 2022-2023\n",
        "\n",
        "Instructor: Manuel Campagnolo mlc@isa.ulisboa.pt\n",
        "\n",
        "Assignment due March 24th, 2023\n",
        "\n",
        "Estimated time to complete the assignment: 2-3 hours."
      ],
      "metadata": {
        "id": "t03zoI_Pg1Ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script below implements the perceptron model for the Titanic tabular data set as discussed in class.\n",
        "\n",
        "\n",
        "The two parts of the assignment are:\n",
        "\n",
        "1. Try changing meta-parameters like the batch size, number of epochs, learning rate, or pre-processing of the numerical data, to try to get the least possible error rate over the validation set. For instance, one possible result with the percepton model is the following, with some set of meta-parameters:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1eiMlyGuLQ0mA1tUw0QNWhTkbHCSqfBU9\" width=\"600\" >\n",
        "\n",
        "2. Adapt the script below to implement a multiple layer *feed-forward neural network*, as described at the end of [ML_overview_with_examples.ipynb](ML_overview_with_examples.ipynb). In order to do this easily, use and adapt if needed the functions defined in the last part of the notebook [Lesson5_edited_for_colab_linear_model_and_neural_net_from_scratch.ipynb](Lesson5_edited_for_colab_linear_model_and_neural_net_from_scratch.ipynb). It is recommend to use a GPU if there are several hidden layers or the number of epochs is large.\n",
        "Then, do as in part 1 of the assigment and try changing the architecture of the network as well as the other meta parameters to obtain a model that performs well over the validation data set. You can describe the architecture by the number of neurons in each hiden layer (from the first to the last).\n",
        "\n",
        "\n",
        "You should write a short report (one single `pdf` file for both parts), where you indicate, for part 1 and part 2, the hyper-parameters and the confusion matrix (by row: true negatives, false positives, false negatives, true positives) as  in the following example. You only need to report three or four results for each part, enough to illustrate some pattern that indicate how you can improve your results. Sort the rows in each table by overall performance.\n",
        "\n",
        "You can add some short comments  for each part.\n",
        "\n",
        "If you wish, you can use a data set other than the Titanic data set. In that case, please add a brief description of the variables and labels.\n",
        "\n",
        "If you can, **please print** a hard copy of your report and bring it to the class. If you're not able to print, then you may send the report by email (subject: assignment PML). \n",
        "\n",
        "---\n",
        "(This is an example: you should choose your own meta-parameters)\n",
        "\n",
        "Name: _______________________\n",
        "\n",
        "Date: ______________________________\n",
        "\n",
        "PART 1 (perceptron)\n",
        "\n",
        "|Batch size | Learning rate | Epochs | Pre-processing| TN, FP, FN, TP| Error_rate|\n",
        "|-||||||\n",
        "|20|0.1|20|Added 1s + standardized | 102, 3, 66, 8|0.3855|\n",
        "|10|0.1|20|Added 1s + standardized | 105, 0, 70, 4|...|\n",
        "|...||||||\n",
        "\n",
        "Comments: ____\n",
        "\n",
        "\n",
        "PART 2 (feed-forward NN)\n",
        "\n",
        "|NN architecture|Batch size | Learning rate | Epochs | Pre-processing| TN, FP, FN, TP|Error_rate|\n",
        "|-|||||||\n",
        "|10,5,10|20|0.1|300|standardized | 88,17,21,53|0.212|\n",
        "|10,10|20|0.1|20|standardized | 102, 3, 66, 8|...|\n",
        "|...|||||||\n",
        "\n",
        "Comments: ____\n"
      ],
      "metadata": {
        "id": "uko3XX_1hHTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "torch.manual_seed(42)\n",
        "\n",
        "B=20 # batch size\n",
        "lr = 0.1 # learning rate\n",
        "iter=20 # number epochs\n",
        "\n",
        "############################################ Reading Titanic numerical data, i.e., X and y\n",
        "var_names=['Age', 'SibSp', 'Parch', 'LogFare', 'Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n",
        "path=Path('/content/drive/MyDrive/AAA/Lesson_5/titanic_data') # adapt to your path\n",
        "X,y=torch.load(path/'titanic_tensor_data_set.ts') # these values are not yet normalized\n",
        "y=y[:,None] # to turn it into a column vector\n",
        "\n",
        "##################################### Create train and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "############################################## Ordinary least squares solution with 0.5 threshold\n",
        "# variables to keep to avoid linear dependencies\n",
        "var_keep=['Age', 'SibSp', 'Parch', 'LogFare', 'Sex_male',  'Pclass_1', 'Pclass_2',  'Embarked_C', 'Embarked_Q']\n",
        "keep=np.isin(var_names,var_keep) # boolean list\n",
        "# \n",
        "from sklearn.linear_model import LinearRegression\n",
        "reg = LinearRegression().fit(X_train[:,keep], y_train)\n",
        "print('coefficients MLR:',reg.intercept_,reg.coef_)\n",
        "y_pred=reg.predict(X_valid[:,keep])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix(y_valid,(y_pred>0.5)))\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "####################################################### Gradient Descent\n",
        "# if you want to standardize X and include an additional additive coefficient to the model:\n",
        "if False: \n",
        "  means = X.mean(dim=1, keepdim=True)\n",
        "  stds = X.std(dim=1, keepdim=True)\n",
        "  X=normalized_data = (X - means) / stds\n",
        "  # add column\n",
        "  ones=torch.ones(X.shape[0]).reshape(X.shape[0],1)\n",
        "  X=torch.cat((ones,X),1)\n",
        "  X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# initial weights\n",
        "def init_coeffs(n_coeff): return (torch.rand(n_coeff,1)-0.5).requires_grad_() # creates a column matrix\n",
        "\n",
        "# defining the function for prediction: the output is a vector of size=nrows(X)\n",
        "def calc_preds(coeffs,X): return  torch.sigmoid(X@coeffs) # using matrix multiplication (aka matmul)\n",
        "\n",
        "# Computing MSE loss for one batch of exemples: the output is a scalar\n",
        "def calc_loss_from_labels(y_pred, y): return torch.mean((y_pred - y) ** 2)\n",
        "\n",
        "# update coeffs\n",
        "def update_coeffs(coeffs, lr):\n",
        "    coeffs.sub_(coeffs.grad * lr)\n",
        "    # zerofy gradients (because they add up)\n",
        "    coeffs.grad.zero_()\n",
        "\n",
        "# compute initial weights as a column matrix\n",
        "n_coeff = X_train.shape[1] # number of columns of X, or X_train, or X_valid\n",
        "coeffs = init_coeffs(n_coeff)\n",
        "\n",
        "# create lists to store losses for each epoch\n",
        "training_losses=[]; validation_losses=[]\n",
        "\n",
        "# epochs\n",
        "for i in range(iter):\n",
        "  # calculating loss as in the beginning of an epoch and storing it\n",
        "    y_pred = calc_preds(coeffs,X_train)\n",
        "    training_losses.append(calc_loss_from_labels(y_pred, y_train).tolist())\n",
        "    y_pred = calc_preds(coeffs,X_valid)\n",
        "    validation_losses.append(calc_loss_from_labels(y_pred, y_valid).tolist())\n",
        "    # mini-batch gradient descent: weight are updated after each batch\n",
        "    for idx_start in np.arange(0,X_train.shape[0],B):\n",
        "        # create batch\n",
        "        batch_X=X_train[idx_start:(idx_start+B),:]\n",
        "        batch_y=y_train[idx_start:(idx_start+B):]\n",
        "        # making a prediction in forward pass\n",
        "        y_pred = calc_preds(coeffs,batch_X)\n",
        "        # calculating the loss between predicted and actual values\n",
        "        loss = calc_loss_from_labels(y_pred, batch_y)\n",
        "        # compute gradient\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            # update coeffs\n",
        "            update_coeffs(coeffs, lr)\n",
        "\n",
        "# predictions and confusion matrix\n",
        "print('coefficients GD:',torch.flatten(coeffs.requires_grad_(False)))\n",
        "y_pred=calc_preds(coeffs,X_valid)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix(y_valid,(y_pred>0.5)))\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "# plot losses along epochs\n",
        "plt.plot(training_losses, '-g',  validation_losses, '-r')\n",
        "plt.gca().legend(('train','validation'))\n",
        "plt.ylim(0, 1)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss (MSE)')\n",
        "#plt.title(\"Train (green) and validation (red) losses\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v1ufDH1Wareo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}