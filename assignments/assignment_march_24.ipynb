{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMg1vOJKuMUEcRC957XM9cp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isa-ulisboa/greends-pml/blob/main/assignments/assignment_march_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical Machine Learning\n",
        "\n",
        "Masters in Green Data Science, ISA/ULisboa, 2022-2023\n",
        "\n",
        "Instructor: Manuel Campagnolo mlc@isa.ulisboa.pt\n",
        "\n",
        "Assignment to return  March 24th, 2023"
      ],
      "metadata": {
        "id": "t03zoI_Pg1Ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script below implements the perceptron model for the Titanic data set as discussed in class.\n",
        "\n",
        "\n",
        "The two parts of the assignment are:\n",
        "\n",
        "1. Try changing hyper-parameters like the batch size, number of epochs, learning rate, or pre-processing of the numerical data, to try to get the least possible error rate over the validation set. For instance, one possible result with the percepton model is the following:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1eiMlyGuLQ0mA1tUw0QNWhTkbHCSqfBU9\" width=\"600\" >\n",
        "\n",
        "2. Adapt the code to implement a multiple layer *feed-forward neural network*, as described at the end of [ML_overview_with_examples.ipynb](ML_overview_with_examples.ipynb). In order to do this easily, use the functions defined in the last part of the notebook [Lesson5_edited_for_colab_linear_model_and_neural_net_from_scratch.ipynb](Lesson5_edited_for_colab_linear_model_and_neural_net_from_scratch.ipynb).\n",
        "Then, do as in part 1 of the assigment and try changing the architecture of the network and other hyper-parameters to obtain a model that performs well over the validation data set.\n",
        "\n",
        "\n",
        "You should write a short report (one single `pdf` file), where you indicate the hyper-parameters and the confusion matrix that you get like in the following example\n",
        "\n",
        "PART 1.\n",
        "\n",
        "Batch size | \n",
        "\n"
      ],
      "metadata": {
        "id": "uko3XX_1hHTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "torch.manual_seed(42)\n",
        "\n",
        "B=20 # batch size\n",
        "lr = 0.1 # learning rate\n",
        "iter=20 # number epochs\n",
        "\n",
        "############################################ Reading Titanic numerical data, i.e., X and y\n",
        "var_names=['Age', 'SibSp', 'Parch', 'LogFare', 'Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n",
        "path=Path('/content/drive/MyDrive/AAA/Lesson_5/titanic_data') # adapt to your path\n",
        "X,y=torch.load(path/'titanic_tensor_data_set.ts') # these values are not yet normalized\n",
        "y=y[:,None] # to turn it into a column vector\n",
        "\n",
        "##################################### Create train and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "############################################## Ordinary least squares solution with 0.5 threshold\n",
        "# variables to keep to avoid linear dependencies\n",
        "var_keep=['Age', 'SibSp', 'Parch', 'LogFare', 'Sex_male',  'Pclass_1', 'Pclass_2',  'Embarked_C', 'Embarked_Q']\n",
        "keep=np.isin(var_names,var_keep) # boolean list\n",
        "# \n",
        "from sklearn.linear_model import LinearRegression\n",
        "reg = LinearRegression().fit(X_train[:,keep], y_train)\n",
        "print('coefficients MLR:',reg.intercept_,reg.coef_)\n",
        "y_pred=reg.predict(X_valid[:,keep])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix(y_valid,(y_pred>0.5)))\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "####################################################### Gradient Descent\n",
        "# if you want to standardize X and include an additional additive coefficient to the model:\n",
        "if False: \n",
        "  means = X.mean(dim=1, keepdim=True)\n",
        "  stds = X.std(dim=1, keepdim=True)\n",
        "  X=normalized_data = (X - means) / stds\n",
        "  # add column\n",
        "  ones=torch.ones(X.shape[0]).reshape(X.shape[0],1)\n",
        "  X=torch.cat((ones,X),1)\n",
        "  X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# initial weights\n",
        "def init_coeffs(n_coeff): return (torch.rand(n_coeff,1)-0.5).requires_grad_() # creates a column matrix\n",
        "\n",
        "# defining the function for prediction: the output is a vector of size=nrows(X)\n",
        "def calc_preds(coeffs,X): return  torch.sigmoid(X@coeffs) # using matrix multiplication (aka matmul)\n",
        "\n",
        "# Computing MSE loss for one batch of exemples: the output is a scalar\n",
        "def calc_loss_from_labels(y_pred, y): return torch.mean((y_pred - y) ** 2)\n",
        "\n",
        "# update coeffs\n",
        "def update_coeffs(coeffs, lr):\n",
        "    coeffs.sub_(coeffs.grad * lr)\n",
        "    # zerofy gradients (because they add up)\n",
        "    coeffs.grad.zero_()\n",
        "\n",
        "# compute initial weights as a column matrix\n",
        "n_coeff = X_train.shape[1] # number of columns of X, or X_train, or X_valid\n",
        "coeffs = init_coeffs(n_coeff)\n",
        "\n",
        "# create lists to store losses for each epoch\n",
        "training_losses=[]; validation_losses=[]\n",
        "\n",
        "# epochs\n",
        "for i in range(iter):\n",
        "  # calculating loss as in the beginning of an epoch and storing it\n",
        "    y_pred = calc_preds(coeffs,X_train)\n",
        "    training_losses.append(calc_loss_from_labels(y_pred, y_train).tolist())\n",
        "    y_pred = calc_preds(coeffs,X_valid)\n",
        "    validation_losses.append(calc_loss_from_labels(y_pred, y_valid).tolist())\n",
        "    # mini-batch gradient descent: weight are updated after each batch\n",
        "    for idx_start in np.arange(0,X_train.shape[0],B):\n",
        "        # create batch\n",
        "        batch_X=X_train[idx_start:(idx_start+B),:]\n",
        "        batch_y=y_train[idx_start:(idx_start+B):]\n",
        "        # making a prediction in forward pass\n",
        "        y_pred = calc_preds(coeffs,batch_X)\n",
        "        # calculating the loss between predicted and actual values\n",
        "        loss = calc_loss_from_labels(y_pred, batch_y)\n",
        "        # compute gradient\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            # update coeffs\n",
        "            update_coeffs(coeffs, lr)\n",
        "\n",
        "# predictions and confusion matrix\n",
        "print('coefficients GD:',torch.flatten(coeffs.requires_grad_(False)))\n",
        "y_pred=calc_preds(coeffs,X_valid)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix(y_valid,(y_pred>0.5)))\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "# plot losses along epochs\n",
        "plt.plot(training_losses, '-g',  validation_losses, '-r')\n",
        "plt.gca().legend(('train','validation'))\n",
        "plt.ylim(0, 1)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss (MSE)')\n",
        "#plt.title(\"Train (green) and validation (red) losses\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v1ufDH1Wareo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}