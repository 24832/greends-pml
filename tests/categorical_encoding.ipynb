{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFJcyT1dBrnYrL8g2jdI3x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isa-ulisboa/greends-pml/blob/main/tests/categorical_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an example of using categorical embedding with PyTorch. The most common approach to create continuous values from categorical data is `nn.Embedding`. It creates a learnable vector representation of the available classes, such that two similar classes (in a specific context) are closer to each other than two dissimilar classes¹.\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "num_classes = 4\n",
        "embedding_size = 10\n",
        "embedding = nn.Embedding(num_classes, embedding_size)\n",
        "class_vector = torch.tensor([1, 0, 3, 3, 2])\n",
        "embedded_classes = embedding(class_vector)\n",
        "```\n",
        "\n",
        "In this example, we create an `nn.Embedding` object with `num_classes` and `embedding_size` as arguments. Then we create a tensor `class_vector` representing our categorical data. Finally, we pass this tensor to our embedding object to get the embedded representation of our categorical data¹.\n",
        "\n",
        "\n",
        "Source: Conversation with Bing, 4/19/2023(1) deep learning - How to create a Pytorch network with mixed categorical .... https://stackoverflow.com/questions/62242396/how-to-create-a-pytorch-network-with-mixed-categorical-and-continuous-matrix-inp Accessed 4/19/2023.\n",
        "(2) Embedding — PyTorch 2.0 documentation. https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html Accessed 4/19/2023.\n",
        "(3) Pytorch: encode categorical feature by using nn.Embedding. https://stackoverflow.com/questions/63844428/pytorch-encode-categorical-feature-by-using-nn-embedding Accessed 4/19/2023."
      ],
      "metadata": {
        "id": "6sR0pXrR63PG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rW7YATyH6PyI"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import string\n",
        "\n",
        "num_classes = 4\n",
        "embedding_size = 10\n",
        "embedding = nn.Embedding(num_classes, embedding_size)\n",
        "class_vector = torch.tensor([1, 0, 3, 3, 2])\n",
        "embedded_classes = embedding(class_vector)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JIp6_nh6Qs2",
        "outputId": "3116bc64-af97-4329-feb9-ad27cb823f70"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2130, -1.0381,  0.0809, -0.9300,  1.2288,  0.4709,  0.6693, -1.2708,\n",
              "         -0.5324,  0.7186],\n",
              "        [-0.3471,  0.5338,  1.0348, -0.7506,  0.5978,  1.0767, -0.8734, -1.7563,\n",
              "          1.5826,  2.2270],\n",
              "        [ 1.3843,  0.4443, -0.6798,  1.2588,  0.2433, -0.0122,  1.3599,  2.1358,\n",
              "         -0.9531, -0.5399],\n",
              "        [ 1.3843,  0.4443, -0.6798,  1.2588,  0.2433, -0.0122,  1.3599,  2.1358,\n",
              "         -0.9531, -0.5399],\n",
              "        [-0.5726,  0.5357,  0.4664, -0.4321, -0.9876,  0.0523, -1.4601,  0.2823,\n",
              "         -0.8536, -0.0932]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string.ascii_uppercase"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "I9OZVYYj6YW3",
        "outputId": "f67b9652-4b33-437e-83be-0f0730d3bd26"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ABCDEFGHIJKLMNOPQRSTUVWXYZ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-hot encoding is a process of converting categorical data variables so they can be provided to machine learning algorithms to improve predictions. In Python, one way to perform one-hot encoding is by using the `pandas` library. Here's an example:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "data = {'fruits': ['apple', 'banana', 'orange', 'banana']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "one_hot_df = pd.get_dummies(df, columns=['fruits'])\n",
        "print(one_hot_df)\n",
        "```\n",
        "\n",
        "This will output:\n",
        "\n",
        "```\n",
        "   fruits_apple  fruits_banana  fruits_orange\n",
        "0             1              0              0\n",
        "1             0              1              0\n",
        "2             0              0              1\n",
        "3             0              1              0\n",
        "```\n",
        "\n",
        "In this example, we create a `DataFrame` with a column `fruits` containing categorical data. Then we use the `pd.get_dummies` function to perform one-hot encoding on the `fruits` column.\n"
      ],
      "metadata": {
        "id": "k6ewnGbxqw87"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LXdIPAdr6lFn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}